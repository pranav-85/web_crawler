# ğŸ•·ï¸ Smart Web Crawler with Summarization & Graph Visualization

This is a multithreaded web crawler built using Python. It crawls web pages from a given starting URL, summarizes the content using a Large Language Model (LLM) like **Gemini API**, stores the results in a local SQLite database and JSON file, and visualizes the crawl structure using a graph.

---

## ğŸš€ Features

- ğŸ”— Crawl internal links starting from a given URL (domain-restricted)
- ğŸ§  Summarize content using Gemini API (LLM-based)
- ğŸ§µ Multi-threaded crawling for faster processing
- ğŸ’¾ Save crawl data to SQLite and JSON
- ğŸŒ Normalize URLs to avoid duplicate visits
- ğŸ“ˆ Visualize crawl graph (pages as nodes, links as edges)
- ğŸ–¥ï¸ CLI dashboard to track progress

---

## ğŸ“‚ Project Structure

```

web-crawler/
â”‚
â”œâ”€â”€ main.py                     # Entry point to run crawler
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ dashboard.py            # Prints crawl progress
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ fetcher.py              # Fetches HTML content
â”‚   â”œâ”€â”€ parser.py               # Extracts title, text, and links
â”‚   â”œâ”€â”€ summarizer.py           # Summarizes content using Gemini or other LLM
â”‚   â”œâ”€â”€ utils.py                # URL normalization and validation
â”‚   â”œâ”€â”€ database.py             # SQLite database operations
â”‚   â””â”€â”€ graph\_builder.py        # Builds and saves crawl graph
â””â”€â”€ data/
â””â”€â”€ results.json            # Summarized data output

````

---

## ğŸ“¥ Installation

1. **Clone the repo**

```bash
git clone https://github.com/yourusername/web-crawler.git
cd web-crawler
````

2. **Create a virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Add your Gemini API Key**
   Set this as an environment variable or place in a `.env` file:

```
GEMINI_API_KEY=your_key_here
```

---

## ğŸ§ª Usage

```bash
python main.py
```

Or update `main.py` to directly call:

```python
crawl("https://example.com", max_pages=10, num_threads=5)
```

---

## ğŸ§  Summary Example (output)

```json
[
  {
    "url": "https://example.com/about",
    "title": "About Us - Example",
    "summary": "This page describes the mission and team behind Example..."
  }
]
```

---

## ğŸ“Š Crawl Graph Output

* A graph image or HTML will be saved as `crawl_graph.html` or `crawl_graph.png` (depending on implementation).
* Pages are nodes, and hyperlinks between them are edges.

---


## ğŸ“š Dependencies

* `requests`, `beautifulsoup4`
* `tqdm` (progress bar)
* `networkx`, `matplotlib`, or `pyvis`
* `openai`, `google-generativeai`, or any LLM API
* `sqlite3` (built-in)
* `concurrent.futures` for multithreading

---

